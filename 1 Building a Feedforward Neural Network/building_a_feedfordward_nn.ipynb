{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Feedfordward NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feed-fordwad propagation from scratch in python\n",
    "* Building back propagation from scratch in python\n",
    "* Building a nn in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a supervised learning algorithm that is loosely inspired by the way the brain functions. Similar to the way neurons are connected to each other in the brain, a neural network takes input, passes it through a function, certain subsequent neurons get excited, and consequently the output is produced.\n",
    "\n",
    "We'll see\n",
    "\n",
    "* Architecture of a neural network\n",
    "* Applications of a neural network\n",
    "* Setting up a feedforward neural network\n",
    "* How forward-propagation works\n",
    "* Calculating loss values\n",
    "* How gradient descent works in back-propagation\n",
    "* The concepts of epochs and batch size\n",
    "* Various loss functions\n",
    "* Various activation functions\n",
    "* Building a neural network from scratch\n",
    "* Building a neural network in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of a simple nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An nn is loosely inspired by the way the bumain brain works. **Technically, it's an improvement over linear and logistic regression as NN introduce multiple non linear measures in estimating the output**.\n",
    "Additionnaly NN provide a great flexibility in modifying the network achitecture to solve the problems across multiple domains leveraging the structured and unstructured data.\n",
    "\n",
    "The more complex the function, the greater the chance that the network has to tune to the data that is given as input, hence the better the accuracy of the predictions.\n",
    "\n",
    "Typical structure of a feed-forward neural network is as follows\n",
    "\n",
    "\n",
    "![Feed forward](feed-forward.png \"Feed forward neura network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer is a collection of one or more nodes (computation units), where each node in a layer is connected  to every other node in the next immediate layer. The input layer is constitued of the __input variables__ that are required to __predict output values__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes in the output layer depends on wether we are trying to predict a continuous variable or a categorical variable. If the output is a continuous variable, the output has one unit. If the output is categorical with $n$ possible classes, there will be $n$ nodes in the output layer. The __hidden mayer__ is use to transform the input layer values into values in a higher dimensional space, so that we can learn more features from the input. The hidden layer transforms the output as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Output transformation by the hidden layer](node.png \"Hidden layers transforms the output as this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the precdeding diagram $x_1, x_2, \\dots, x_n$ are the independant variables, and $x_0$ is the __bias term (similar to the way we have bias in linear/logistic regression)__\n",
    "\n",
    "Note that $w_1, w_2, \\dots , w_n$ are the __weights__ given to each input variables. If $a$ is one of the units in the hidden layer, it will be equal to the following:\n",
    "\n",
    "$ a = f(\\sum^{N}_{i = 0})w_ix_i $ \n",
    "\n",
    "$f = $ **activation function that is used to apply non-linearity on top of the** **sum-product** of the input and their corresponding weight values. Higher non linearity can be achieved by having more that one hidden layer\n",
    "\n",
    "In sum, **a neural network is a collection of weights assigned to hidden layers connecting them**. The collection is organized into three main parts : ***The input layer***, ***the hidden layer*** and ***the output layer***.\\\n",
    "Note that you can have $n$ hidden layers, with the **term deep learning implying multiple hidden layers**.\\\n",
    "Hidden layers are necessary when **the neural network has to make sense of something really complicated, contextual or not obvious** such as image recognition. The intermediate layers (layers that are not input or ouput) are known as hidden, since they are practically not visible(there's more on how to visualize them in chapter 4, *Building a Deep CNN*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network basically means *calibrating all of the weights in a neural network by repeating two key steps*: **forward-propagation**  and **back-propagation**.\n",
    "\n",
    "In *forward-propagation*, we **apply a set of weights to the input data**, **pass it through the hidden layer**, **perform the nonlinear activation** on the hidden layer output, and then **connect the hidden layer to the output layer** by **multiplying the hidden layer node values with another set of weights**. For the **first forward-propagation, the values of the weights are initialized randomly**.\n",
    "\n",
    "In *back-propagation*, we try to **decrease the error** by **measuring the margin of error of output and then adjust weight accordingly**. *Neural networks repeat both forward- and back-propagation to predict an output until the weights are calibrated*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN can be architected in multiple ways\n",
    "\n",
    " ![Architecture of neural networks](architecture.png \"Architecture of neural networks\")\n",
    "\n",
    "\n",
    "\n",
    "|Architecture    | Exemple                                                                                                 |\n",
    "|----------------|---------------------------------------------------------------------------------------------------------|\n",
    "|   One-to-many  |  **The input is an image** and **the output is a caption for the image**                                        |\n",
    "|   Many-to-one  | **The input is a movie review** (multiple words) and **the output is the sentiment** associated with the review |\n",
    "|   Many-to-many | **Machine translation of a sentence in one language to a sentence in another language**                     |\n",
    "\n",
    "Apart from the preceding points, neural networks are also in a position to understand the content in an image and detect  the position where the content is located using an architecture named __Convolutional Neural Network (CNN)__\n",
    "\n",
    " ![Architecture of convolutional nn](cnn_architecture.png \"Architecture of cnn\")\n",
    "\n",
    " Here, we saw examples of recommender systems, image analysis, text analysis, and audio analysis, and we can see that neural networks give us the flexibility to solve a problem using multiple architectures, resulting in increased adoption as the number of applications increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward propagation from scratch in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a strong foundation of how *feed-forward propagation* works, we'll go through a toy example of training a nn where the input to then nn is $(1, 1)$ and the corresponding output is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy : our nn will have one hidden layer (with neurons) __connecting the input to the output layer__\n",
    "\n",
    " ![nn with one hidden layer](nn_1_hidden_layer.png \"nn with 1 hidden layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the hidden layer unit values\n",
    "\n",
    "We now assing weights to **all of the connections**. These values are selected randomly(base on gaussian distro) since it's the **first time we're forward-propagating**.\n",
    "\n",
    "In this specific case, let's start with initial weights that are between 0 and 1, but note that the final weights after the training process of a neural network **don't need to be between a specific set of values**.\n",
    "\n",
    "![Inital weigthed](initial_weighted_nn.png \"Initial weighted\")\n",
    "\n",
    "In the next step, we perform multplication of the input with weights to calculate the values of hidden units in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer's unit values are obtained as follows:\n",
    "\n",
    "$h_1 = 1 \\times 0.8 + 1 \\times 0.2 = 1$\n",
    "\n",
    "$h_2 = 1 \\times 0.4 + 1 \\times 0.9 = 1.3$\n",
    "\n",
    "$h_3 = 1 \\times 0.3 + 1 \\times 0.5 = 0.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer's unit values are also shown in the following diagram:\n",
    "\n",
    "![Inital weigthed](initial_weighted_nn_with_node_values.png \"Initial weighted\")\n",
    "\n",
    "Note that in the preceding output we calculated the hidden values. For simplicity, we excluded the __bias terms that need to be added at each unit of a hidden layer__ \n",
    "\n",
    "Now we'll pass the hidden layer values through an activation function so that we attain non-linearity in our output\n",
    "\n",
    "_NB: If we don't apply the activation function in the hidden layer, the nn becomes a giant linear connection from the input to output_ \n",
    "\n",
    "##### Applying the activation function\n",
    "Activation functions are applied at multiple layers of network. They're use so that we achieve high non-linearity in input, which can be useful in modeling complex relations between the input and output.\n",
    "\n",
    "The different activation are as follows:\n",
    "\n",
    "![activation function](activation_functions.png \"Activation function\")\n",
    "\n",
    "For our example, let's use the sigmoid function for activation. The sigmoid function looks like graphically:\n",
    "![Sigmoid function](sigmoid.png \"Sigmoid function\")\n",
    "\n",
    "By applying sigmoid activation, $S(x)$, _to the three hidden=layer sum_\n",
    ", we get the values\n",
    "\n",
    "$final~h_1 = S(1.0) = 0.73$\n",
    "\n",
    "$final~h_2 = S(1.3) = 0.78$\n",
    "\n",
    "$final~h_3 = S(0.8) = 0.69$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the output layered values\n",
    "\n",
    "As hte hidden layer values are known, we will be calculating the output layer value. In the following diagram, we have the hidden layer values connected to the output through the __randomly-initialized weight values__. Using the __hidden layer values__ and the __weight values__, we will calculate __the output values__ for the following network:\n",
    "\n",
    "![Output values](output_value_1.png \"Output values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the sum product of the hidden layer values and weight values to calculate the output value. For simplicity, we excluded the bias terms that need to be added at each unit of the hidden layer:\n",
    "\n",
    "$0.73 \\times 0.3 + 0.79 \\times 0.5 + 0.69 \\times 0.9 = 1.235$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are shown in the following diagram:\n",
    "\n",
    "![Output values](output_value_2.png \"Output values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we started with a random set of weights, the value of the output neuron is very different from the target, in this case by +1.235 (since the target is 0).\n",
    "\n",
    "##### Calculating the loss values\n",
    "\n",
    "Loss values (alternatively called cost functions) are values that optimize in a nn. In order to understand how loss values get calculated, let's look at 2 scenarios:\n",
    "\n",
    "* Continuous variable prediction\n",
    "* Categorical variable prediction\n",
    "\n",
    "##### Calculating loss during continuous variable prediction\n",
    "\n",
    "Typically, when the variable is a continuous one, the loss value is calculated as the squared error, that is, we try to minimize the mean squared error by varying the weight values associated with the neural network:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum^{m}_{i = 1}(h(\\theta)(x^{(i)}) - y^{i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding equation, $y(i)$ is the actual value of output, $h(x)$ is the transformation that we apply on the input $(x)$ to obtain a predicted value of $y$, and $m$ is the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating loss during categorical variable prediction**\n",
    "When the variable to predict is a discrete one (that is, there are only a few categories in the variable), we typically use a categorical cross-entropy loss function. When the variable to predict has two distinct values within it, the loss function is binary cross-entropy, and when the variable to predict has multiple distinct values within it, the loss function is a categorical cross-entropy.\n",
    "\n",
    "Here is binary cross-entropy\n",
    "$(ylog(p)+(1−y)log(1−p))$\n",
    "\n",
    "Here is categorical cross-entropy: \n",
    "\n",
    "$-\\sum^{M}_{n=1}y_nlog(p_n)$\n",
    "\n",
    "$y$ is the actual value of output $p$, is the predicted value of the output  and n is the total number of data points. For now, let's assume that the outcome that we are predicting in our toy example is continuous. In that case, the loss function value is the mean squared error, which is calculated as follows:\n",
    "\n",
    "$error = 1.235^2 = 1.52$\n",
    "\n",
    "In the next step, we will try to minimize the loss function value using back-propagation (which we'll learn about in the next section), where we update the weight values (which were initialized randomly earlier) to minimize the loss (error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we learned about performing the following steps on top of the input data to come up with error values in forward-propagation (the code file is available as Neural_network_working_details.ipynb in GitHub):\n",
    "\n",
    "* Initialize weights randomly\n",
    "* Calculate the hidden layer unit values by multiplying input values with weights\n",
    "* Perform activation on the hidden layer values\n",
    "* Connect the hidden layer values to the output layer\n",
    "* Calculate the squared error loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to calculate the squared error loss values across all data points is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    pre_hidden = np.dot(inputs, weights[0] + weights[1])\n",
    "    hidden = 1/(1 + np.exp(-pre_hidden))\n",
    "    pred_out = np.dot(hidden, weights[2] + weights[3])\n",
    "    squared_error = (np.square(pred_out - outputs))\n",
    "    return squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding function, we take the input variable values, weights (randomly initialized if this is the first iteration), and the actual output in the provided dataset as the input to the feed-forward function.\n",
    "We calculate the hidden layer values by performing the matrix multiplication (dot product) of the input and weights. Additionally, we add the bias values in the hidden layer, as follows:\n",
    "```python\n",
    "pre_hidden = np.dot(inputs,weights[0])+ weights[1]\n",
    "```\n",
    "\n",
    "The preceding scenario is valid when weights[0] is the weight value and weights[1] is the bias value, where the weight and bias are connecting the input layer to the hidden layer.\n",
    "\n",
    "Once we calculate the hidden layer values, we perform activation on top of the hidden layer values, as follows:\n",
    "```python\n",
    "hidden = 1/(1+np.exp(-pre_hidden))\n",
    "```\n",
    "We now calculate the output at the hidden layer by multiplying the output of the hidden layer with weights that connect the hidden layer to the output, and then adding the bias term at the output, as follows:\n",
    "\n",
    "```python\n",
    "pred_out = np.dot(hidden, weights[2]) + weights[3]\n",
    "```\n",
    "Once the output is calculated, we calculate the squared error loss at each row, as follows:\n",
    "\n",
    "```python\n",
    "squared_error = (np.square(pred_out - outputs))\n",
    "```\n",
    "In the preceding code, pred_out is the predicted output and outputs is the actual output.\n",
    "\n",
    "We are then in a position to obtain the loss value as we forward-pass through the network.\n",
    "\n",
    "While we considered the sigmoid activation on top of the hidden layer values in the preceding code, let's examine other activation functions that are commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tanh\n",
    "\n",
    "The tanh activation of a value (the hidden layer unit value) is calculated as follows:\n",
    "```python\n",
    "def tanh(x):\n",
    "    return (exp(x)-exp(-x))/(exp(x)+exp(-x))\n",
    "```\n",
    "##### ReLu\n",
    "\n",
    "The Rectified Linear Unit (ReLU) of a value (the hidden layer unit value) is calculated as follows:\n",
    "\n",
    "```python\n",
    "def relu(x):\n",
    "    return np.where(x>0,x,0)\n",
    "```\n",
    "\n",
    "##### Linear\n",
    "\n",
    "The linear activation of a value is the value itself.\n",
    "\n",
    "##### Softmax\n",
    "\n",
    "Typically, softmax is performed on top of a vector of values. This is generally done to determine the probability of an input belonging to one of the n number of the possible output classes in a given scenario. Let's say we are trying to classify an image of a digit into one of the possible 10 classes (numbers from 0 to 9). In this case, there are 10 output values, where each output value should represent the probability of an input image belonging to one of the 10 classes.\n",
    "\n",
    "The softmax activation is used to provide a probability value for each class in the output and is calculated explained in the following sections:\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "```\n",
    "\n",
    "Apart from the preceding activation functions, the loss functions that are generally used while building a neural network are as follows.\n",
    "\n",
    "##### Mean squared error\n",
    "\n",
    "The error is the difference between the actual and predicted values of the output. We take a square of the error, as the error can be positive or negative (when the predicted value is greater than the actual value and vice versa). Squaring ensures that positive and negative errors do not offset each other. We calculate the mean squared error so that the error over two different datasets is comparable when the datasets are not the same size.\n",
    "\n",
    "The mean squared error between predicted values $(p)$ and actual values $(y)$ is calculated as follows:\n",
    "\n",
    "```python\n",
    "def mse(p, y):\n",
    "    return np.mean(np.square(p - y))\n",
    "```\n",
    "\n",
    "The mean squared error is typically used when trying to predict a value that is continuous in nature.\n",
    "\n",
    "##### Mean absolute error\n",
    "\n",
    "The mean absolute error works in a manner that is very similar to the mean squared error. The mean absolute error ensures that positive and negative errors do not offset each other by taking an average of the absolute difference between the actual and predicted values across all data points.\n",
    "\n",
    "The mean absolute error between the predicted values $(p)$ and actual values $(y)$ is implemented as follows:\n",
    "\n",
    "```python\n",
    "def mae(p, y):\n",
    "    return np.mean(np.abs(p-y))\n",
    "```\n",
    "\n",
    "Similar to the mean squared error, the mean absolute error is generally employed on continuous variables.\n",
    "\n",
    "Categorical cross-entropy\n",
    "\n",
    "Cross-entropy is a measure of the difference between two different distributions: actual and predicted. It is applied to categorical output data, unlike the previous two loss functions that we discussed.\n",
    "\n",
    "Cross-entropy between two distributions is calculated as follows:\n",
    "\n",
    "$-(ylog_{2}p + 1(1 - y)log_2(1 -p))$\n",
    "\n",
    "y is the actual outcome of the event and p is the predicted outcome of the event.\n",
    "\n",
    "Categorical cross-entropy between the predicted values $(p)$ and actual values $(y)$ is implemented as follows:\n",
    "\n",
    "```python\n",
    "def cat_cross_entropy(p, y):\n",
    "     return -np.sum((y*np.log2(p)+(1-y)*np.log2(1-p)))\n",
    "```\n",
    "\n",
    "Note that categorical cross-entropy loss has a high value when the predicted value is far away from the actual value and a low value when the values are close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building back-propagation from scratch in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In forward-propagation, we connected the input layer to the hidden layer to the output layer. In back-propagation, we take the reverse approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We **change each weight within the neural network by a small amount – one at a time**. A change in the weight value will have an impact on the final loss value (either increasing or decreasing loss). We'll update the weight in the direction of decreasing loss.\n",
    "\n",
    "Additionally, in **some scenarios, for a small change in weight, the error increases/decreases considerably**, while in **some cases the error decreases by a small amount**.\n",
    "\n",
    "**By updating the weights by a small amount and measuring the change in error that the update in weights leads to**, we are able to do the following:\n",
    "\n",
    "* **Determine the direction of the weight update**\n",
    "* **Determine the magnitude of the weight update**\n",
    "Before implementing back-propagation, let's understand one additional detail of neural networks: **the learning rate**.\n",
    "\n",
    "Intuitively, **the learning rate helps us to build trust in the algorithm**. For example, when deciding on the magnitude of the weight update, we would potentially not change it by a huge amount in one go, but take a more careful approach in updating the weights more slowly.\n",
    "\n",
    "This results in obtaining stability in our model; we will look at how the learning rate helps with stability in the next chapter.\n",
    "\n",
    "The whole process by which we update weights to reduce error is called a **gradient-descent technique**\n",
    "\n",
    "**Stochastic gradient descent** is the means by which error is minimized in the preceding scenario. More intuitively, *gradient* stands for difference(which is the difference between actual and predict) and *descent* means reduce.\n",
    "**Stochastic** stands for the selection of number of random samples based on which a decision is taken.\n",
    "\n",
    "Apart from stochastic gradient descent, there are many other optimization techniques that help to optimize for the loss values; the different optimization techniques will be discussed in the next chapter\n",
    "\n",
    "Back propagation works as follows:\n",
    "\n",
    "* Calculates the overall cost function from the feedforward process.\n",
    "* Varies all the weights (one at a time) by a small amount.\n",
    "* Calculates the impact of the variation of weight on the cost function.\n",
    "* Depending on whether the change has an increased or decreased the cost (loss) value, it updates the weight value in the direction of loss decrease. And then repeats this step across all the weights we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the preceding steps are performed n number of times, it essentially results in $n$ _epochs_.\n",
    "\n",
    "In order to further cement our understanding of back-propagation in neural networks, let's start with a known function and see how the weights could be derived:\n",
    "\n",
    "For now, we will have the known function as $y = 2x$, where we try to come up with the weight value and bias value, which are 2 and 0 in this specific case:\n",
    "\n",
    "| x   | y   |\n",
    "| --- | --- |\n",
    "| 1   | 2   |\n",
    "| 2   | 4   |\n",
    "| 3   | 6   |\n",
    "| 4   | 8   |\n",
    "\n",
    "If we formulate the preceding dataset as a linear regression, $(y = a \\times x+b)$, where we are trying to calculate the values of $a$ and $b$ (which we already know are $2$ and $0$, but are checking how those values are obtained using gradient descent), let's randomly initialize the $a$ and $b$ parameters to values of $1.477$ and $0$ (the ideal values of which are $2$ and $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will build the back-propagation algorithm by hand so that we clearly understand how weights are calculated in a neural network. In this specific case, we will build a simple neural network where there is no hidden layer (thus we are solving a regression equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Dataset initialization\n",
    "x = [[1], [2], [3], [4]]\n",
    "y = [[2], [4], [6], [8]]\n",
    "\n",
    "# 2 Initialize the weight and bias values randomly(we have only one wight and one bias\n",
    "# value as we are trying to identify the optimal values of a and b in y = a * x + b equation)\n",
    "w = [[1.477867], [0.]]\n",
    "\n",
    "# 3 Define the feed forward network and calculate the squared error loss value\n",
    "import numpy as np\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    out = np.dot(inputs, weights[0] + weights[1])\n",
    "    squared_error = (np.square(out - outputs))\n",
    "    return squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we performed a matrix multiplication of the input with the randomly-initialized weight value and summed it up with the randomly-initialized bias value.\\\n",
    "Once the value is calculated, we calculate the squared error value of the difference between the actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Increase each weight and bias value by a very small amount $(0.0001)$ and calculate the squared error loss value one at a time for each of the weight and bias updates.\n",
    "\n",
    "**If the squared error loss value decreases as the weight increases, the weight value should be increased**. **The magnitude by which the weight value should be increased is proportional to the amount of loss value the weight change decreases by**.\n",
    "\n",
    "Additionally, **ensure that you do not increase the weight value as much as the loss decrease caused by the weight change**, but weight it down with a factor called the learning rate. This ensures that the loss decreases more smoothly (there's more on how the learning rate impacts the model accuracy in the next chapter).\n",
    "\n",
    "In the following code, we are creating a function named `update_weights`, which performs the back-propagation process to update weights that were obtained in step 3. We are also mentioning that the function needs to be run for `epochs` number of times (where `epochs` is a parameter we are passing to `update_weights` function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def update_weights(inputs, outputs, weights, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # 5 Pass thhe input through a feed_forward network to calculate\n",
    "        # the loss with the initial set of weights\n",
    "        org_loss = feed_forward(inputs, outputs, weights)\n",
    "        # 6 Ensure that you deepcopy the list of weights, as they will be\n",
    "        # manipulated in further steps, and hence deepcopy takes care of any\n",
    "        # issues resulting from the change in the child variable impacting\n",
    "        # the parent variable that is pointing to\n",
    "        wts_temp = deepcopy(weights)\n",
    "        wts_temp_2 = deepcopy(weights)\n",
    "        # 7 Loop through all the weight values, one at a time, and change\n",
    "        # them by a small value .0001\n",
    "        for i in range(len(weights)):\n",
    "            wts_temp[-(i+1)] += .0001\n",
    "            # 8 caclulate the update feed forward loss when the weight is updated by\n",
    "            # small amount. Calculate the change in loss due t ho the small change in\n",
    "            # input. Divide the change in loss by the number of input, as we want to\n",
    "            # calculate the mean squared error accross all the input samples we have\n",
    "            loss = feed_forward(inputs=inputs, outputs=outputs, weights=wts_temp)\n",
    "            delta_loss = np.sum(org_loss - loss / (.0001 * len(inputs)))\n",
    "            # NB : Updating the weight by a small value and then calculating its impact\n",
    "            # on loss value is  equivalent to performing a derivative with respect to\n",
    "            # change in weight.\n",
    "\n",
    "            # 9  Update the weights by the change in loss that they are causing. Update\n",
    "            # weigths slowly by multiplying the change in loss by a very small number\n",
    "            # 0.01, which is the learning rate parameter\n",
    "            wts_temp_2[-(i+1)] += delta_loss*.01\n",
    "            wts_temp = deepcopy(weights)\n",
    "        # 10 The updated weights and bias are returned\n",
    "        weights = deepcopy(wts_temp_2)\n",
    "    return wts_temp_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the other parameters in a neural network is the **batch size** considered in calculating the loss values. \n",
    "\n",
    "In the preceding scenario, we considered all the data points in order to calculate the loss value. However, i**n practice, when we have thousands (or in some cases, millions) of data points**, the incremental contribution of a greater number of data points while calculating loss value would follow the law of diminishing returns and hence we would be using a batch size that is much smaller compared to the total number of data points we have.\n",
    "\n",
    "The typical batch size considered in building a model is anywhere between $32$ and $1,024$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we built a regression formula $(Y = a \\times x + b)$ where we wrote a function to identify the optimal values of $a$ and $b$. In this section, we will **build a simple neural network with a hidden layer that connects the input to the output on the same toy dataset** that we worked on in the previous section.\n",
    "* The input is connected to a hidden layer that has three units\n",
    "* The hidden layer is connected to the output, which has one unit in output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\[NulledPremium.com] Neural Networks with Keras Cookbook Over 70 recipes leveraging\\Codes\\Building a Feedforward Neural Network\\building_a_feedfordward_nn.ipynb Cellule 48\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m wts_new2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# 5 Run the function an epoch number of times to update\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# the weights an epoch number of times\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m update_weights(x, y, w, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\[NulledPremium.com] Neural Networks with Keras Cookbook Over 70 recipes leveraging\\Codes\\Building a Feedforward Neural Network\\building_a_feedfordward_nn.ipynb Cellule 48\u001b[0m in \u001b[0;36mupdate_weights\u001b[1;34m(inputs, outputs, weights, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, weight \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mndenumerate(layer):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mprint\u001b[39m(index, weight)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     wts_new[\u001b[39m-\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)][index] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mweights_new:\u001b[39m\u001b[39m'\u001b[39m, wts_new)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/%5BNulledPremium.com%5D%20Neural%20Networks%20with%20Keras%20Cookbook%20Over%2070%20recipes%20leveraging/Codes/Building%20a%20Feedforward%20Neural%20Network/building_a_feedfordward_nn.ipynb#Y105sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     loss \u001b[39m=\u001b[39m feed_forward(inputs, outputs, wts_new)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# 1 Define the aataset and import packages\n",
    "from copy import deepcopy # used so that the original value doesn't change\n",
    "                          # when its clone value changes\n",
    "import numpy as np\n",
    "\n",
    "x = [[1], [2], [3], [4]]\n",
    "y = [[2], [4], [6], [8]]\n",
    "\n",
    "# 2 Initialize the weights and bias values randomly. The hidden layer has\n",
    "# three units in it. Hence, there are a total of 3 wights values and 3 bias\n",
    "# values- one corresponding to eahc of the hidden units\n",
    "\n",
    "# Additionally, the final layer has one unit that is connected to the three\n",
    "# units of the hidden layer. Hence, a total of three weights and one bias\n",
    "# dictate the value of the output layer.\n",
    "# The randomly-initialized weights\n",
    "w = [[[-0.82203424, -0.9185806 , 0.03494298]], [0., 0., 0.], [[ 1.0692896 ],[ 0.62761235],[-0.5426246 ]], [0]]\n",
    "# 3 Implement the feed-forward network where the hidden layer has a ReLU activation\n",
    "# in it\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    pre_hidden = np.dot(inputs, weights[0]) + weights[1]\n",
    "    hidden = np.where(pre_hidden < 0, 0, pre_hidden)\n",
    "    out = np.dot(hidden, weights[2]) + weights[3]\n",
    "    squared_error = (np.square(out - outputs))\n",
    "    return squared_error\n",
    "\n",
    "# 4 Define the back-propagation function similarly to what we did in the\n",
    "# previous section. The only difference is that we now have to update the weights\n",
    "# in more layers\n",
    "# we are calculating the original loss at the start of an epoch:\n",
    "def update_weights(inputs, outputs, weights, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        org_loss = feed_forward(inputs, outputs, weights)\n",
    "        # we are copying weights into two sets of weight variables so\n",
    "        # that they can be reused in a later code: \n",
    "        wts_new = deepcopy(weights)\n",
    "        wts_new2 = deepcopy(weights)\n",
    "        # we are updating each weight value by a small amount and then\n",
    "        # calculating the loss value corresponding to the updated weight\n",
    "        # value (while every other weight is kept unchanged). Additionally,\n",
    "        # we are ensuring that the weight update happens across all weights\n",
    "        # and also across all layers in a network.\n",
    "        #\n",
    "        # The change in the squared loss (del_loss) is attributed to the\n",
    "        # change in the weight value. We repeat the preceding step for all\n",
    "        # the weights that exist in the network\n",
    "        for i, layer in enumerate(reversed(weights)):\n",
    "            for index, weight in np.ndenumerate(layer):\n",
    "                print(index, weight)\n",
    "                wts_new[-(i+1)][index] += 0.0001\n",
    "                print('weights_new:', wts_new)\n",
    "                loss = feed_forward(inputs, outputs, wts_new)\n",
    "                del_loss = np.sum(org_loss - loss)/(0.0001*len(inputs))\n",
    "                # The weight value is updated by weighing down by the learning\n",
    "                # rate parameter – a greater decrease in loss will update weights\n",
    "                # by a lot, while a lower decrease in loss will update the weight\n",
    "                # by a small amount\n",
    "                wts_new2[-(i+1)][index] += del_loss*0.01\n",
    "                wts_new = deepcopy(weights)\n",
    "            # Finally, we return the updated weights\n",
    "            weights = deepcopy(wts_new2)\n",
    "    return wts_new2\n",
    "\n",
    "# 5 Run the function an epoch number of times to update\n",
    "# the weights an epoch number of times\n",
    "update_weights(x, y, w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a NN in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we built a neural network from scratch, that is, we wrote functions that perform forward-propagation and back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it...\n",
    "We will be building a neural network using the Keras library, which provides utilities that make the process of building a complex neural network much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our 1st model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 3)                 6         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 Instantiate a model that can be called sequentially to\n",
    "# add further layers on top of it. The Sequential method enables\n",
    "# us to perform the model initialization exercise\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# 2.Add a dense layer to the model. A dense layer ensures the\n",
    "# connection between various layers in a model. In the following code,\n",
    "# we are connecting the input layer to the hidden layer:\n",
    "model.add(tf.keras.layers.Dense(3, activation='relu', input_shape=(1,)))\n",
    "\n",
    "# In the dense layer initialized with the preceding code, we ensured\n",
    "# that we provide the input shape to the model (we need to specify\n",
    "# the shape of data that the model has to expect as this is the first\n",
    "# dense layer).\n",
    "\n",
    "# Additionally, we mentioned that there will be three connections\n",
    "# made to each input (three units in the hidden layer) and also that\n",
    "# the activation that needs to be performed in the hidden layer is\n",
    "# the ReLu activation.\n",
    "\n",
    "# 3 Connect the hidden layer to the output layer:\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Note that in this dense layer, we don't need to specify the input\n",
    "# shape, as the model would already infer the input shape from the\n",
    "# previous layer. \n",
    "# Also, given that each output is one-dimensional, our output layer has one unit and the activation that we are performing is the linear activation.\n",
    "# The model summary can now be visualized as follows:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding output confirms our discussion in the previous section: that there will be a total of six parameters in the connection from the input layer to the hidden layer—three weights and three bias terms—we have a total of six parameters corresponding to the three hidden units. In addition, three weights and one bias term connect the hidden layer to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibrao\\miniconda3\\envs\\computer-vision\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 4 Compile the model. This ensures that we define the loss\n",
    "# function and the optimizer to reduce the loss function\n",
    "# and the learning rate corresponding to the optimizer (we will look at\n",
    "# different optimizers and loss functions in next chapter):\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we specified that the optimizer is the stochastic gradient descent that we learned about in the previous section and the learning rate is 0.01. Pass the predefined optimizer and its corresponding learning rate as a parameter and reduce the mean squared error value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 30.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24473e652a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "# 5 Fit the model. Update the weights so that the model is a better fit\n",
    "model.fit(np.array(x), np.array(y), epochs=1, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method expects that it receives two NumPy arrays: an input array and the corresponding output array. Note that `epochs` represents the number of times the total dataset is traversed through, and `batch_size` represents the number of data points that need to be considered in an iteration of updating the weights. Furthermore,` verbose` specifies that the output is more detailed, with information about losses in training and test datasets as well as the progress of the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(1, 3) dtype=float32, numpy=array([[-0.5479983 , -0.18417871, -0.32838565]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
       " array([[-0.39945894],\n",
       "        [ 0.5317837 ],\n",
       "        [-0.35060352]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.09999999], dtype=float32)>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 Extract the weight values. The order in which the weight values are presented is obtained by calling the weights method on top of\n",
    "# the model, as follows:\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preceding output, we see that the order of weights is the three weights (`kernel`) and three bias terms in the `dense_1` layer (which is the connection between the input to the hidden layer) and the three weights (`kernel`) and one bias term connecting the hidden layer to the `dense_2` layer (the output layer).\n",
    "\n",
    "Now that we understand the order in which weight values are presented, let's extract the values of these weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.5479983 , -0.18417871, -0.32838565]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.39945894],\n",
       "        [ 0.5317837 ],\n",
       "        [-0.35060352]], dtype=float32),\n",
       " array([0.09999999], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the weights are presented as a list of arrays, where each array corresponds to the value that is specified in the model.weights output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the output we are observing here matches with the output we obtaining while hand-building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 190ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09999999],\n",
       "       [0.09999999]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Predict the output for a new set of input using the predict method:\n",
    "x1 = [[5], [6]]\n",
    "model.predict(np.array(x1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('computer-vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d43d35bdc3653585fb433c6fc0216c1bb3b2c7e59f7b629c50617613cb181de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
